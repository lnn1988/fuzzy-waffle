# 卡夫卡:一种分布式的日志处理信息系统

### 摘要

日志处理已成为互联网公司数据管道中的重要组成部分。 我们介绍Kafka，一个分布式消息传递系统，我们开发该系统是为了以低延迟收集和传递大量日志数据。 我们的系统结合了现有的日志聚合器和消息传递系统的想法，适用于脱机和联机环境。 我们在Kafka中做出了许多非常规但实用的设计选择，以使我们的系统高效且可扩展。 与两种流行的消息传递系统相比，实验结果表明Kafka具有更卓越的性能。 我们已经在生产中使用Kafka已有一段时间了，它每天要处理数百GB的新数据。

### Keywords
消息传递，分布式，日志处理，吞吐量，联机

## 1. 简介

所有大规模的互联网公司都会生成大量的“日志”数据，通常包括：
1. 登录、浏览、点击、喜欢、共享、评论或查询类似的用户活动行为事件
2. 操作指标，比如服务调用堆栈、调用延迟、错误或者其他系统指标如 CPU、内存、网络或者硬盘使用量

日志数据已经变成了跟踪用户参与度、系统指标的关键组成部分。但是，互联网的最新趋势是使活动数据通过生产数据管道直接用于站点功能。包括：

1. 搜索相关性
2. 通过主题流行性或者共现程度提供的建议
3. 广告目标人群定位
4. 防止滥用行为，比如垃圾邮件或者爬虫过滤
5. 新闻提要功能以供用户的状态更新给他的朋友或者相关的人

日志数据的这种实时生产使用给数据系统带来了新的挑战，因为其数据量比“实际”数据大几个数量级。 例如，搜索，推荐和广告经常需要计算精确的点击率，这不仅会为每个用户点击生成日志记录，还会为每个页面上数十个未被点击的项目生成日志记录。 每天，中国移动收集5–8 TB的电话记录[11]，而Facebook收集近6 TB的各种用户活动事件[12]。
许多用于处理此类数据的早期系统都依靠物理方式将日志文件从生产服务器上拷贝下来进行分析。 近年来，已经建立了几种专门的分布式日志聚合器，包括Facebook的Scribe [6]，Yahoo的Data Highway [4]和Cloudera的Flume [3]。 这些系统主要用于收集日志数据并将其加载到数据仓库或Hadoop [8]中以供离线使用。 在LinkedIn（一个社交网站）上，我们发现，除了传统的离线分析之外，我们还需要以不超过几秒钟的延迟来支持上述大多数实时应用程序。

我们建立了一种用于日志处理的新型消息传递系统，称为Kafka [18]，该系统结合了传统日志聚合器和消息传递系统的优势。 一方面，Kafka是分布式且可扩展的，并提供高吞吐量。 另一方面，Kafka提供类似于消息传递系统的API，并允许应用程序实时使用日志事件。 Kafka已开源，并已在LinkedIn上成功用于生产中超过6个月。 因为我们可以利用单个软件来在线和离线使用所有类型的日志数据，所以它极大地简化了我们的基础架构。 本文的其余部分安排如下。 我们将在第2节中回顾传统的消息传递系统和日志聚合器。在第3节中，我们描述Kafka的体系结构及其关键设计原则。 我们将在第4节中介绍在LinkedIn上部署Kafka的情况，并在第5节中介绍Kafka的性能结果。在第6节中，我们讨论未来的工作并进行总结。

## 2. 相关工作

传统的企业消息传递系统[1] [7] [15] [17]已经存在了很长时间，并且经常作为处理异步数据流的事件总线发挥关键作用。但是，由于某些原因，它们往往不适合日志处理。首先，企业系统提供的功能不匹配。这些系统通常专注于提供丰富的交付保证。例如，IBM Websphere MQ [7]具有事务支持，允许应用程序原子地将消息插入多个队列。 JMS [14]规范允许在使用后确认每个单独的消息，这可能会导致混乱。这样的交付保证对于收集日志数据通常是过度的。例如，偶尔失去一些综合浏览量事件肯定不是致命的。这些不需要的功能往往会增加API和这些系统的基础实现的复杂性。其次，许多系统没有像其主要设计约束那样专注于吞吐量。例如，JMS没有API允许生产者将多个消息显式批处理为单个请求。这意味着每个消息都需要完整的TCP / IP往返，这对于这个领域的吞吐量要求是不可行的。第三，这些系统在分布式支持方面薄弱。没有简单的方法可以在多台计算机上进行分区和存储消息。最后，许多消息传递系统假定消息被收集到之后将立即消耗掉，因此未消耗消息的队列总是很小。如果允许累积消息，对于脱机使用的情形，它们的性能将大大降低，例如，数据仓库应用程序，它们会定期进行大负载而不是连续使用。

在过去的几年中，已经建立了许多专业的日志聚合器。 Facebook使用名为Scribe的系统。每个前端计算机都可以通过套接字将日志数据发送到一组Scribe计算机。每台Scribe机器都会汇总日志条目，并定期将它们转储到HDFS [9]或NFS设备。雅虎的data highway项目具有类似的数据流。一组计算机聚合来自客户端的事件并推出“分钟”文件，然后将其添加到HDFS。 Flume是由Cloudera开发的相对较新的日志聚合器。它支持可扩展的“管道”和“接收器”，使得流式日志数据非常灵活，并且融合了分布式支持。但是，大多数这些系统是为脱机消费日志数据而构建的，并且经常不必要地向使用者公开实现细节（例如“分钟文件”）。此外，它们中的大多数使用“push”模型，其中broker将数据转发给消费者。在LinkedIn，我们发现“pull”模型更适合我们的应用程序，因为每个消费者都可以以其可以维持的最大速率检索消息，并避免被推送速度超出其处理的能力从而导致消息淹没。pull模型还可以轻松地回放消费者，我们将在第3.2节的末尾讨论这种好处。最近，雅虎！研究开发了一种新的分布式发布/订阅系统，称为HedWig [13]。 HedWig具有高度的可扩展性和可用性，并提供强大的耐用性保证。但是，它主要用于存储数据存储的提交日志。

## 3. Kafka 架构以及设计原理

由于现有系统的限制，我们开发了一种新的基于消息的日志聚合器Kafka。 我们首先介绍Kafka中的基本概念。 主题定义特定类型的消息流。 生产者可以将消息发布到主题。 然后，已发布的消息将存储在一组称为broker的服务器上。 消费者可以订阅来自broker的一个或多个主题，并通过从broker获取数据来消费订阅的消息。
消息从概念上讲很简单，我们试图使Kafka API同样简单以反映这一点。 我们没有介绍确切的API，而是提供了一些示例代码来说明如何使用API。 生产者的示例代码如下。 一条消息被定义为仅包含字节的有效负载。 用户可以选择喜欢的序列化方法来编码消息。 为了提高效率，生产者可以在单个发布请求中发送一组消息。

**Sample producer code:** 

```java
producer = new Producer(…); 
message = new Message(“test message str”.getBytes()); 
set = new MessageSet(message); 
producer.send(“topic1”, set);
```

为了订阅主题，消费者首先为该主题创建一个或多个消息流。 发布给该主题的消息将平均分配到这些子流中。 有关Kafka如何分发消息的详细信息，将在第3.2节中介绍。 每个消息流在正在生成的连续消息流上提供迭代器接口。 然后，使用者遍历流中的每个消息并处理消息的有效负载。 与传统的迭代器不同，消息流迭代器永远不会终止。 如果当前没有更多消息可使用，则迭代器将阻塞，直到将新消息发布到该主题为止。 我们既支持点对点传递模型（其中多个使用者共同使用一个主题中所有消息的一个副本），也支持发布/订阅模型（其中多个使用者各自检索其自己的主题副本）。

**Sample consumer code:**

```java
streams[] = Consumer.createMessageStreams(“topic1”, 1)
for (message : streams[0]) 
{ 
	bytes = message.payload(); 
	// do something with the bytes 
}
```

Kafka的总体体系结构如图1所示。由于Kafka实际上是分布式的，因此Kafka集群通常由多个broker组成。 为了平衡负载，一个主题分为多个分区，每个代理存储一个或多个这些分区。 多个生产者和消费者可以同时发布和检索消息。 在第3.1节中，我们描述了代理上单个分区的布局以及我们选择的一些设计选择，以使访问分区更加有效。 在3.2节中，我们描述了生产者和消费者如何在分布式环境中与多个broker进行交互。 我们将在第3.3节中讨论Kafka的交付保证。

![截屏2019-10-22下午3.13.30](https://tva1.sinaimg.cn/large/006y8mN6ly1g86zzejsihj309g076754.jpg)



### 3.1 一个分区上的效率

我们在Kafka做出了一些决策，以提高系统效率。
**简单的存储**：Kafka的存储层非常简单。 主题的每个分区都对应一个逻辑日志。 物理上，日志是由一组大小近似相同（例如1GB）的分段文件实现的。 每次生产者将消息发布到分区时，代理都将消息简单地附加到最后一个分段文件。 为了获得更好的性能，我们仅在发布了可配置数量的消息或经过一定时间后才将分段文件刷新到磁盘。 消息仅在刷新后才显示给使用者。

与典型的消息系统不同，存储在Kafka中的消息没有明确的消息ID。而是，每个消息都通过其在日志中的逻辑偏移量来寻址。这样避免了维护将消息ID映射到实际消息位置的辅助，寻找密集型随机访问索引结构的开销。请注意，我们的消息ID始终在增加，但不是连续的。要计算下一条消息的ID，我们必须将当前消息的长度添加到其ID中。从现在开始，我们将交替使用消息ID和偏移量。
使用者始终按顺序使用来自特定分区的消息。如果使用者确认特定的消息偏移量，则表示使用者已经接收了所有在分区中的该偏移量之前的消息。在背后，使用者正在向broker发出异步请求，以使数据缓冲区可供应用程序使用。每个拉取请求都包含使用的消息的偏移量和要提取的可接受的字节数。每个broker将内存中的偏移量列表排序，包括每个分段文件中第一条消息的偏移量。broker通过搜索偏移量列表来定位所请求消息所驻留的分段文件，并将数据发送回使用者。使用者接收到一条消息后，计算下一条要使用的消息的偏移量，并在下一个请求请求中使用它。 Kafka日志和内存索引的布局如图2所示。每个框显示一条消息的偏移量。

![截屏2019-10-24下午12.34.24](https://tva1.sinaimg.cn/large/006y8mN6ly1g896n1ypd8j309y08hgmg.jpg)

**高效传输**：我们非常关注数据在Kafka的进与出。之前，我们已经展示了生产者可以在单个发送请求中提交一组消息。尽管最终用户API一次迭代一条消息，但在幕后，来自用户的每个拉取请求还可以检索到一定大小（通常为数百KB）的多条消息。
我们做出的另一个非常规选择是避免在Kafka层上将消息显式缓存在内存中。相反，我们依赖于基础文件系统页面缓存。这具有避免双重缓冲的主要好处-消息仅被高速缓存在页面高速缓存中。这样做还有一个好处，即使重新启动broker进程，也可以保留热缓存。由于Kafka根本不缓存进程中的消息，因此在垃圾回收内存方面几乎没有开销，因此可以使用基于VM的语言进行高效实现。最后，由于生产者和使用者都按顺序访问分段文件，使用者总是会落后于生产者一小部分，因此正常的操作系统缓存试探法非常有效（特别是直写式缓存和预读）。我们已经发现，生产和消耗都具有与数据大小成线性关系的一致性能，最大可达数TB的数据。

此外，我们为消费者优化了网络访问。 Kafka是一个多用户系统，单个消息可能会被不同的消费者应用程序多次使用。 一个典型的从本地文件向远程套接字发送字节的方法包括以下步骤：

1. 从存储介质读取数据到OS中的页面缓存

2. 将页面缓存中的数据复制到应用程序缓冲区

3. 将应用程序缓冲区复制到另一个内核缓冲区

4. 将内核缓冲区发送到套接字。 

这包括4个数据复制和2个系统调用。 在Linux和其他Unix操作系统上，存在一个sendfile API [5]，它可以直接将字节从文件通道传输到套接字通道。 这样通常可以避免步骤2和步骤3中引入2个副本和1个系统调用。 Kafka利用sendfile API有效地将日志段文件中的字节从broker传递到使用者。

**无状态broker**：与大多数其他消息传递系统不同，在Kafka中，有关每个消费者已消费多少的信息不是由broker而是由消费者自己维护。 这样的设计减少了broker的很多复杂性开销。 但是，由于broker不知道是否所有订阅者都已使用消息，因此删除消息变得很棘手。 Kafka通过将简单的基于时间的SLA用于保留策略来解决此问题。 如果消息在broker中的保留时间超过一定时间（通常为7天），则会自动删除该消息。 该解决方案在实践中效果很好。 大多数消费者（包括离线消费者）每天，每小时或实时完成消费。 Kafka的性能不会随着数据量的增加而降低，这一事实使得这种长期保留成为可能。

此设计还有一个很重要的好处。 消费者可以倒回旧的偏移量并重新使用数据。 这违反了队列的通用约定，但是事实证明这是许多消费者的基本功能。 例如，当使用者中的应用程序逻辑出现错误时，错误修复后，应用程序可以回放某些消息。 这对于将ETL数据加载到我们的数据仓库或Hadoop系统中特别重要。 作为另一示例，所消费的数据可以仅周期性地被刷新到持久性存储（例如，全文本索引器）。 如果使用者崩溃，则未刷新的数据将丢失。 在这种情况下，使用者可以检查未刷新消息的最小偏移量，并在重新启动后从该偏移量中重新使用。 我们注意到，在pull模型中支持回放消费者要比push模型容易得多。

### 3.2 分布式协调
现在我们介绍生产者和消费者在分布式环境中的行为。 每个生产者可以将消息发布到随机选择的分区或者由分区键和分区函数确定的分区。 我们更关注消费者与broker的互动方式。Kafka具有消费组的概念。 每个消费者组由一个或多个消费者组成，共同订阅某个主题，即，每个消息仅传递给该组中的一个消费者。 不同的消费者组各自独立地消费整个订阅消息集，并且不需要跨消费者组进行协调。 同一组中的使用者可以处于不同的进程中，也可以位于不同的机器上。 我们的目标是在用户之间平均分配存储在broker中的消息，而不会引入过多的协调开销。

我们的第一个决定是使主题内的分区作为最小的并行度单位。 **这意味着在任何给定时间，来自一个分区的所有消息仅由每个消费者组中的单个消费者使用。** 如果我们允许同一个组中的多个消费者同时使用一个分区，那么他们将必须协调谁使用哪些消息，这需要锁和状态维护开销。 相反，在我们的设计中，消耗过程仅在消耗者重新平衡负载时才需要协调，这种情况很少发生。 为了使负载真正达到平衡，一个主题中需要的分区数目要比每组中的消费者数目多得多。 我们可以通过对主题进行过度划分来轻松实现这一目标。

我们做出的第二个决定是没有中央的master节点，而是让消费者以分散的方式在彼此之间进行协调。 添加主服务器会使系统复杂化，因为我们必须进一步考虑master服务器故障。 为了促进协调，我们使用了高度可用的协调服务Zookeeper [10]。 Zookeeper有一个非常简单的类似于文件系统的API。 可以创建路径，设置路径的值，读取路径的值，删除路径并列出路径的子级。 它做一些更有趣的事情：

* 可以在路径上注册观察者，并在路径的子代或路径的值发生更改时得到通知； 
* 可以临时创建一条路径（与持久性相反），这意味着如果创建的客户端不存在，则该路径将由Zookeeper服务器自动删除； 
* zookeeper将其数据复制到多个服务器，这使数据高度可靠且可用。

Kafka使用Zookeeper执行以下任务：1. 检测broker和消费者的添加和删除，2. 在上述事件发生时在每个消费者中触发重新平衡过程，3. 维护消费关系并跟踪每个分区的消耗的偏移量。具体来说，每个broker或消费者启动时，会将其信息存储在Zookeeper中的broker或消费者注册表中。broker注册表包含broker的主机名和端口，以及存储在其中的主题和分区集。消费者注册表包括消费者所属的消费者组及其订阅的主题集。每个消费者组都与Zookeeper中的所有ownership注册表和偏移量注册表关联。所有ownership注册表对每个订阅的分区都有一个路径，路径值是该分区当前使用的使用者的ID（我们使用使用者拥有该分区的术语）。偏移量注册表为每个订阅的分区存储该分区中最后消耗的消息的偏移量。

在Zookeeper，对于broker注册表，使用者注册表和所有权注册表，注册的路径是临时的，对于偏移量注册表是持久的。 如果broker失败，则该broker上的所有分区都会自动从broker注册表中删除。 消费者的故障导致它丢失在消费者注册表中的条目以及在ownership注册表中拥有的所有分区。 每个使用者都在broker注册表和消费者注册表上都注册了Zookeeper监视程序，并且每当代理集或消费者组发生更改时，都将收到通知。

在消费者的初始启动过程中，或者观察者通知消费者有关broker/消费者更改的消息时，消费者将启动重新平衡过程以确定应从中使用的新分区子集。 算法1中描述了该过程。通过从Zookeeper中读取broker和消费者注册表，消费者首先计算可用于每个已订阅主题T的分区集（PT）和订阅T的消费者集（CT）。 按照范围分区将PT个分区集分为|CT|个数据集块并为每一个数据集块确定一个消费者。 消费者对于选择的每个分区，都会在所有所有者注册表中将自己写为该分区的新所有者。 最后，消费者开始一个线程，从偏移注册表中存储的偏移开始，从每个拥有的分区中提取数据。 随着从分区中提取消息，使用者将定期更新偏移量注册表中的最新消耗的偏移量。

![截屏2019-10-24下午3.13.22](https://tva1.sinaimg.cn/large/006y8mN6ly1g89b8c98zfj30ac08ojsb.jpg)

当一个组中有多个消费者时，将通知每个broker或消费者变更。 但是，在消费者处通知的发出时间可能略有不同。 因此，一个消费者有可能尝试获得仍由另一个消费者拥有的分区的所有权。 发生这种情况时，第一个消费者只需释放其当前拥有的所有分区，稍等片刻，然后重试重新平衡过程。 实际上，重新平衡过程通常只需重试几次即可稳定下来。创建新的使消费组时，偏移量注册表中没有可用的偏移量。 在这种情况下，使用我们在broker上提供的API，消费者将从每个订阅分区上可用的最小或最大偏移量（取决于配置）开始。

## 3.3 传输保证

通常，Kafka仅保证至少一次交付。 恰好一次交付通常需要两阶段提交，对于我们的应用不是必要的。 大多数情况下，一条消息仅一次发送给每个消费者组。 但是，如果消费者进程崩溃而没有完全关闭，则接管有故障的消费者拥有的那些分区的消费者进程可能会得到一些重复的消息，这些消息是在最后一次偏移成功提交给Zookeeper之后的。 如果应用程序关心重复项，则它必须使用我们返回给使用者的偏移量或消息中的某些唯一键来添加自己的重复数据删除逻辑。 与使用两阶段提交相比，这通常是一种更具成本效益的方法。

Kafka保证将来自单个分区的消息按顺序传递给消费者。 但是，不能保证来自不同分区的消息的顺序。

为了避免日志损坏，Kafka将每个消息的CRC存储在日志中。 如果broker上存在任何 I/O 错误，Kafka将运行恢复过程以删除带有不一致CRC的消息。 在消息级别使用CRC还可以使我们在产生或消费消息之后检查网络错误。如果broker发生故障，则存储在其上尚未使用的任何消息将变得不可用。 如果broker上的存储系统被永久损坏，则所有未使用的消息将永远丢失。 将来，我们计划在Kafka中添加内置复制，以将每个消息冗余地存储在多个broker上。



## 4. Kafka 在 LinkedIn 的应用

在本节中，我们描述了LinkedIn在如何使用Kafka。 图3显示了我们部署的简化版本。 我们有一个Kafka集群，并与运行面向用户服务的每个数据中心共置一处。 前端服务生成各种日志数据，并将其分批发布到本地Kafka 的 broker。 我们依靠硬件负载平衡器将发布请求平均分配给Kafka broker集。 Kafka的在线消费者在同一数据中心内的服务中运行。



![截屏2019-10-24下午6.59.33](https://tva1.sinaimg.cn/large/006y8mN6ly1g89hrdo25qj30ac0a5t9m.jpg)




